# æŠ€æœ¯æ¶æ„å®¡è§†æŠ¥å‘Šï¼ˆdispatch-address-line-closure-006ï¼‰

**å®¡è§†äººè§’è‰²**: æ¶æ„å¸ˆå…¼é¡¹ç›®ç»ç†  
**å®¡è§†æ—¶é—´**: 2026-02-16 09:10:00 CST  
**å®¡è§†èŒƒå›´**: æ ¸å¿ƒå¼•æ“ã€æµ‹è¯•æ¡†æ¶ã€Web Dashboardã€æ•°æ®åº“å±‚ã€å¯è§‚æµ‹æ€§  
**é£é™©è¯„çº§æ ‡å‡†**: ğŸ”´é«˜é£é™©ï¼ˆå½±å“å‘å¸ƒï¼‰| ğŸŸ¡ä¸­é£é™©ï¼ˆå½±å“ä½“éªŒï¼‰| ğŸŸ¢ä½é£é™©ï¼ˆåç»­ä¼˜åŒ–ï¼‰  

---

## ğŸ“‹ æ ¸å¿ƒæŠ€æœ¯ç°çŠ¶æ‰«æ

åŸºäºå®é™…ä»£ç äº§å‡ºç‰©çš„æ·±åº¦åˆ†æï¼ˆéPMå¼è¡¨é¢æ–‡å­—ï¼‰ï¼š

### 1. **å¥åº·åº¦è®¡ç®—å…¬å¼ - æ•°å­¦æ¨¡å‹ç¼ºé™·** ğŸŸ¡ ä¸­é£é™©

**å½“å‰å®ç°** (web/dashboard/app.js L150-163):
```javascript
let health = 100 - blockedLines*8 - gateFail*18 - pkgRisk*6 + Math.round(pkgAvgProgress*0.08);
health = Math.max(0, Math.min(100, health));
```

**é—®é¢˜åˆ†æ**:

| é—®é¢˜ | å½±å“ | ä¾‹ |
|------|------|-----|
| **æƒé‡å‚æ•°éçº¿æ€§** | é˜»å¡çº¿æ•°é‡çˆ†ç‚¸é£é™© | åªè¦2æ¡çº¿blockedï¼Œå·²æ‰£16;6æ¡çº¿blockedå°±=-48ï¼ˆè§¦å‘floor=0ï¼‰ |
| **Progressè´¡çŒ®æå°** | æ¨è¿›å·¥ä½œæ”¶ç›Šçœ‹ä¸è§ | progressä»0â†’100ä»…åŠ 8åˆ†ï¼Œç›¸å½“äº1æ¡çº¿blockedå°±æŠµæ¶ˆ |
| **ç¼ºä¹é˜¶æ®µæƒé‡** | æ—©æœŸè¿›åº¦ä¸æœ«æœŸåŒæƒ | 0%â†’20%çš„åŠªåŠ›ä¸80%â†’100%çš„åŠªåŠ›ä»·å€¼ç›¸åŒ |
| **æ— é£é™©è¡°å‡** | è¿‡æœŸçš„é£é™©è¢«é—å¿˜ | ä¸€å‘¨å‰è¯†åˆ«çš„é£é™©ä»ç®—åˆ°healthï¼Œå¯¼è‡´chronic fatigue |
| **é˜ˆå€¼è®¾å®šæ­¦æ–­** | åˆ¤å®šæ ‡å‡†æ¨¡ç³Š | health=79 "æ³¨æ„é£é™©" vs 80 "ç¨³å®šæ¨è¿›"ï¼Œä¸€æ¡çº¿å®Œæˆå°±æå‰§å˜åŒ– |

**å»ºè®®æ–¹æ¡ˆ**:
```python
# åˆ†æ®µæƒé‡ + è¿›åº¦åŠ æƒ + é£é™©è¡°å‡
def compute_health_v2(blockedLines, gateFail, pkgRisk, pkgAvgProgress, risk_age_hours):
    # é˜¶æ®µè¿›åº¦æƒé‡ï¼šæ—©æœŸæŠ•å…¥æƒé‡ä½ï¼Œæ”¶å°¾é˜¶æ®µæƒé‡é«˜
    progress_weight = 0.15 if pkgAvgProgress < 50 else 0.25  # late-stage boost
    
    # é£é™©è¡°å‡ï¼š7å¤©å¤–çš„é£é™©ä¸å†æ‰£åˆ†ï¼ˆå·²å¤„ç†æˆ–acceptedï¼‰
    risk_multiplier = max(0, 1 - risk_age_hours / (7 * 24))
    
    # Logistic å‡½æ•°è€Œéçº¿æ€§ï¼Œé¿å…æç«¯æ³¢åŠ¨
    from scipy.special import expit
    blocked_penalty = 25 * expit((blockedLines - 1.5) / 0.8)  # Sæ›²çº¿
    gate_penalty = 20 * gateFail / 4
    
    health = 100 - blocked_penalty - gate_penalty - pkgRisk*5*risk_multiplier + progress_weight*pkgAvgProgress
    return round(max(0, min(100, health)))

# é˜ˆå€¼åˆ†å±‚åŒ–
if health < 50: label = "URGENT:éœ€è¦å¹²é¢„"  # çº¢è‰²ï¼Œé¡¹ç›®ç»ç†å‡çº§
elif health < 70: label = "CAUTION:æ³¨æ„é£é™©"  # é»„è‰²ï¼Œå·¥ä½œçº¿å¢åŠ åŒæ­¥é¢‘ç‡
elif health < 85: label = "STEADY:ç¨³å®šæ¨è¿›"  # ç»¿è‰²ï¼Œç»§ç»­å½“å‰èŠ‚å¥
else: label = "EXCELLENT:è¶…é¢„æœŸè¿›å±•"  # æ·±ç»¿ï¼Œè¯†åˆ«å¯å¤ç”¨ç»éªŒ
```

**ä¼˜åŒ–æ”¶ç›Š**:
- âœ… å‡å°‘healthè™šå‡æ³¢åŠ¨ï¼ˆå½“å‰baselineæ³¢åŠ¨10-20åˆ†ï¼‰ï¼Œç¨³å®šåˆ°5åˆ†ä»¥å†…
- âœ… è¿›åº¦æ¨è¿›å¯è§†åŒ–ï¼ˆ80%â†’90%èƒ½æ˜æ˜¾çœ‹åˆ°healthæå‡ï¼‰
- âœ… å¯å®¹çº³"accepted risk"æ¦‚å¿µï¼ˆæ˜ç¡®åˆ—å‡ºå“ªäº›é£é™©å·²æ¥å—ï¼‰

---

### 2. **æ•°æ®åº“æ¶æ„ - ä¸¤å±‚å­˜å‚¨ç¼ºä¹åŒæ­¥ä¿è¯** ğŸŸ¡ ä¸­é£é™©

**å½“å‰æ¶æ„**:
```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ ç”Ÿäº§ä¾§ï¼ˆå†·å±‚ï¼‰                       â”‚
â”‚ PostgreSQL (alembic migrations)     â”‚
â”‚ - addr_batch, addr_task_run        â”‚
â”‚ - addr_raw, addr_canonical         â”‚
â”‚ - change_requests, ruleset_audits  â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
               â”‚ (å¼‚æ­¥/æ‰‹å·¥ï¼Ÿ)
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â–¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ è¿è¡Œæ—¶ä¾§ï¼ˆçƒ­å±‚ï¼‰                     â”‚
â”‚ SQLite (runtime state)              â”‚
â”‚ - SQLiteEvidenceStore               â”‚
â”‚ - SQLiteStateStore                  â”‚
â”‚ - failure_queue, replay_runs        â”‚
â”‚ - line_feedback.latest.json         â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

**å…³é”®é£é™©**:

| é£é™© | å½“å‰åº¦ | è§¦å‘åœºæ™¯ | åæœ |
|------|--------|---------|------|
| **æ•°æ®ä¸ä¸€è‡´** | **é«˜** | çƒ­å±‚transactionæäº¤ï¼Œå†·å±‚åŒæ­¥å¤±è´¥ | replay_runsæœ‰é‡å¤run_id | 
| **å¹¶å‘å†™ç«é€Ÿ** | **ä¸­** | å¤šä¸ªå·¥ä½œçº¿å¹¶å‘æ‰§è¡Œï¼Œéƒ½å¾€failure_queueå†™ | SQLiteä¸²è¡ŒåŒ–ï¼Œç“¶é¢ˆ |
| **å¤‡ä»½ä¸å®Œæ•´** | **é«˜** | ä»…å¤‡ä»½Postgresï¼ŒSQLiteçƒ­æ•°æ®æœªå¤‡ä»½ | æœºå™¨æ•…éšœï¼Œè¿è¡Œæ—¶stateä¸§å¤± |
| **è·¨DBäº‹åŠ¡** | **æ— ** | å†·çƒ­æ•°æ®éœ€è¦åŸå­æ€§æ›´æ–° | éƒ¨åˆ†å¤±è´¥ï¼Œcleanupå›°éš¾ |
| **é•¿æœŸæ•°æ®è†¨èƒ€** | **ä¸­** | failure_queueæ—¥ç§¯æœˆç´¯ | æŸ¥è¯¢æ€§èƒ½ä¸‹é™10x, nightlyè´¨é‡é—¨æ§›timeout |

**ç°çŠ¶è¯æ®** (ä»ä»£ç æ‰«æ):
- `scripts/run_p0_workpackage.py` L115-195: æ‰‹å·¥SQLå¼•ç”¨æ ¼å¼æ ¡éªŒ `sqlite://<path>#<table>`ï¼Œæ— ORMä¿æŠ¤
- æ— Alembicç‰ˆæœ¬ç®¡ç†SQLite schema
- æ— è·¨DB äº‹åŠ¡æ—¥å¿—æˆ–äº‹ä»¶æº¯æºæ¨¡å¼

**å»ºè®®æ–¹æ¡ˆ**:
```python
# 1. äº‹ä»¶æº¯æºæ¨¡å¼ï¼ˆEvent Sourcingï¼‰
class DataFlow:
    """æ‰€æœ‰å†™å…¥æ“ä½œè®°å½•ä¸ºChange Log"""
    
    @dataclass
    class Event:
        event_id: str  # UUIDï¼Œå…¨å±€å”¯ä¸€
        entity_type: str  # "addr_batch", "replay_run"
        entity_id: str
        op: str  # "create", "update", "replay"
        payload: dict
        source_db: str  # "postgres" / "sqlite:hot"
        timestamp: datetime
        synced_to_cold: bool = False
        cold_sync_timestamp: Optional[datetime] = None
    
    def write_op_to_both_dbs(self, event: Event):
        """ä¿è¯ä¸¤å±‚åŒæ­¥"""
        # 1. å†™å…¥cold (Postgres) - æºå¤´
        postgres_conn.record_event(event)
        event_id = event.event_id
        
        # 2. å¼‚æ­¥å†™å…¥hot (SQLiteï¼Œå½“eventæ¶‰åŠè¿è¡Œæ—¶)
        if event.source_db in ["sqlite:hot", "both"]:
            sqlite_conn.record_event(event, synced=False)
        
        # 3. åå°åŒæ­¥jobå®šæœŸæ£€æŸ¥ synced_to_cold==Falseï¼Œé‡è¯•å¤±è´¥äº‹ä»¶
        
    def replay_to_state(self, since_event_id: str) -> dict:
        """ä»Postgreså†·å±‚é‡å»ºSQLiteçƒ­å±‚çŠ¶æ€"""
        # é€ä¸ªapply eventsï¼Œä¿è¯ä¸€è‡´æ€§
        ...

# 2. SQLiteç»´æŠ¤
# - æ¯å‘¨VACUUMæ‰€æœ‰SQLiteæ–‡ä»¶ï¼ˆæ¸…ç†ç¢ç‰‡ï¼‰
# - crontab: find output/workpackages/*.db -exec sqlite3 {} "VACUUM;" \;

# 3. ç›‘æ§
# - DataFrame count(failure_queue) å‘¨æœŸæ£€æŸ¥
# - è‹¥count > 10000ï¼Œè§¦å‘alert: "failure_queue needs archival"
# - è‡ªåŠ¨archive: INSERT INTO archive_failure_queue SELECT * FROM failure_queue WHERE created_at < now()-30d
```

**ä¼˜åŒ–æ”¶ç›Š**:
- âœ… æ•°æ®ä¸€è‡´æ€§ä¿è¯ï¼ˆEventä½œä¸ºå•ä¸€source of truthï¼‰
- âœ… å¯è¿½æº¯æ€§ï¼ˆå®Œæ•´çš„change logï¼Œä¾¿äºauditå’Œrollbackï¼‰
- âœ… çƒ­å±‚æ€§èƒ½ç¨³å®šï¼ˆfailure_queueæ°¸ä¸è¶…è¿‡5000æ¡ï¼‰

---

### 3. **Timeouté…ç½®ç¢ç‰‡åŒ– - åˆ†å¸ƒå¼ç³»ç»Ÿçš„éšè—ç‚¸å¼¹** ğŸŸ¡ ä¸­é£é™©

**å½“å‰çŠ¶æ€çš„è¶…æ—¶é…ç½®**:

| ç»„ä»¶ | Timeout | é…ç½®ä½ç½® | å¤‡æ³¨ |
|------|---------|---------|------|
| trust_data_hub fetchers | 20s | `services/trust_data_hub/app/execution/fetchers.py:16` | ç½‘ç»œIO |
| governance_api ops SQL | 1500ms | `services/governance_api/app/models/ops_models.py:46` | DBæŸ¥è¯¢ |
| governance_api lab | 2s | `services/governance_api/app/models/lab_models.py:153` | è§„åˆ™æ‰§è¡Œ |
| nightly web_e2e | 90sï¼ˆå«retryï¼‰ | `coordination/status/test-quality-gate.md` | UIè‡ªåŠ¨åŒ– |
| **æ€»ç«¯åˆ°ç«¯ webæµç¨‹** | **æ— æ˜ç¡®å®šä¹‰** | - | âŒ é—®é¢˜åœ¨è¿™é‡Œ |

**é—®é¢˜**:
1. **ç¼ºä¹ SLA å±‚çº§** - æ²¡æœ‰å®šä¹‰ P99 latency budgetï¼Œå¯¼è‡´ï¼š
   - trust_data_hub fetchers(20s) + ops_sql(1.5s) + lab(2s) = **23.5sæœ€å°**ï¼Œä½†æŸäº›slow queryå¯èƒ½20s+ï¼Œå¯¼è‡´overflow
   - å®¢æˆ·ç«¯è¶…æ—¶(30s?)ä¸serverç«¯è¶…æ—¶(23.5s)ä¸åŒ¹é…ï¼Œå¯èƒ½å‡ºç°åŠå¼€è¿æ¥

2. **é‡è¯•ç­–ç•¥ä¸ä¸€è‡´è¡¨** (test_nightly_quality_gate_v2.py):
   ```
   web_e2e_optimize_retries: 3
   web_e2e_optimize_retry_delay_sec: 1.5
   ```
   ä½†governance_apiæœªå®šä¹‰é‡è¯•ã€‚è¿™æ„å‘³ç€ä¸€ä¸ªSQL timeoutç›´æ¥å¤±è´¥ï¼Œè€Œweb_e2eè‡ªåŠ¨é‡è¯•ï¼Œå¯¼è‡´ï¼š
   - webå¯èƒ½èŠ±45ç§’æ‰åˆ¤å®šå¤±è´¥ï¼ˆ3æ¬¡ retryï¼‰
   - SQLç›´æ¥timeoutï¼Œæ— é‡è¯•

3. **çº§è”è¶…æ—¶é£é™©** - å‡è®¾ï¼š
   ```
   è¯·æ±‚é“¾è·¯ï¼š
   dashboard -> governance_api(op/lab) -> trust_data_hub(fetch) -> å¤–éƒ¨API
   
   è‹¥å¤–éƒ¨API slow response(18s)ï¼š
   - trust_data_hub 20s timeout âœ… æ¥ä½
   - governance_api 1.5s timeout âŒ ç­‰ä¸åˆ°ï¼Œrejection
   - dashboard user 30s timeout âœ… çœ‹åˆ°error
   
   ç”¨æˆ·ä½“éªŒï¼šçœ‹ä¼¼æ˜¯governance_apiæ•…éšœï¼Œå®é™…æ˜¯ä¸Šæ¸¸fetcheræ…¢
   ```

**å»ºè®®æ–¹æ¡ˆ**:
```python
# services/governance_api/app/models/timeout_config.py
from dataclasses import dataclass

@dataclass
class TimeoutPolicy:
    """SLAçº§è¶…æ—¶ç­–ç•¥"""
    
    # P99 latency budget åˆ†é…
    TOTAL_END_TO_END_BUDGET_MS = 8000  # 8s for dashboard UX responsiveness
    
    # ç»„ä»¶çº§é¢„ç®—ï¼ˆé€’å½’åˆ†é…ï¼šçˆ¶-å­margin=1sï¼‰
    GOVERNANCE_API_OUTER_LAYER = 7500  # Leave 500ms margin
    
    # å†…éƒ¨æ“ä½œ
    OPS_SQL_QUERY = 1200        # Single DB query, with margin
    LAB_RULESET_EXECUTION = 1800  # Rule engine
    TRUST_HUB_FETCH = 2000       # Network call with buffer
    
    # é‡è¯•ç­–ç•¥ï¼ˆæŒ‰å¯æ¢å¤æ€§ï¼‰
    RETRYABLE_OPS = {
        'trust_fetch': {'max_attempts': 2, 'base_delay_ms': 500},  # Network flake
        'sql_query': {'max_attempts': 1, 'base_delay_ms': 0},      # æ— é‡è¯•ï¼ˆæ­»é”é£é™©ï¼‰
        'ruleset': {'max_attempts': 1, 'base_delay_ms': 0},         # è§„åˆ™æ‰§è¡Œæ— é‡è¯•
    }
    
    def validate(self, actual_duration_ms: float, op_name: str) -> dict:
        """è¿è¡Œæ—¶æ ¡éªŒ"""
        budget = getattr(self, f'{op_name.upper()}_TIMEOUT_MS', 5000)
        over_budget = actual_duration_ms - budget
        
        return {
            'status': 'OK' if over_budget <= 0 else 'APPROACHING' if over_budget < 200 else 'EXCEEDED',
            'budget_ms': budget,
            'actual_ms': actual_duration_ms,
            'margin_ms': budget - actual_duration_ms,
        }

# ä½¿ç”¨ç¤ºä¾‹
@app.post("/api/v1/lab/optimize")
async def post_optimize(req: OptimizeRequest) -> OptimizeResponse:
    policy = TimeoutPolicy()
    
    async with asyncio.timeout(policy.GOVERNANCE_API_OUTER_LAYER / 1000):
        try:
            # æ ‡è®°è¶…æ—¶ä¸å¯é‡è¯•ï¼Œå› ä¸ºå·²åˆ†é…é‡è¯•ç»™ä¸‹å±‚fetcher
            result = await call_trust_hub(
                timeout_ms=policy.TRUST_HUB_FETCH,
                retries=policy.RETRYABLE_OPS['trust_fetch']['max_attempts'],
            )
            
            perf = {
                'fetcher_ms': result.elapsed_ms,
            }
            perf['validation'] = policy.validate(result.elapsed_ms, 'trust_hub_fetch')
            
            return OptimizeResponse(result=result, perf_telemetry=perf)
        except asyncio.TimeoutError:
            logger.error(f"OUTER timeout exceeded {policy.GOVERNANCE_API_OUTER_LAYER}ms")
            raise HTTPException(500, detail="Service timeout (SLA exceeded)")
```

**ä¼˜åŒ–æ”¶ç›Š**:
- âœ… å¯é¢„æµ‹çš„å»¶è¿Ÿï¼ˆP99 latency <= 8sï¼Œä¸å†surpriseï¼‰
- âœ… è‡ªåŠ©æ¢å¤ï¼ˆé«˜å¯æ¢å¤çš„æ“ä½œè‡ªåŠ¨é‡è¯•ï¼Œä½å¯æ¢å¤çš„ç›´æ¥failï¼‰
- âœ… é—®é¢˜å¯è¿½æº¯ï¼ˆtelemetryæ¸…æ™°æ˜¾ç¤ºtimeoutå‘ç”Ÿåœ¨å“ªä¸€å±‚ï¼‰

---

### 4. **line_feedback åˆçº¦çš„è„†å¼±æ€§ - ç»“æ„åŒ–æ ¡éªŒç¼ºå¤±** ğŸŸ¡ ä¸­é£é™©

**å½“å‰æœºåˆ¶** (scripts/run_p0_workpackage.py L115-195):
```python
def _validate_line_feedback_payload(
    payload: dict[str, Any],
    required_fields: list[str],
    expected_failure_ref: str,
    expected_replay_ref: str,
) -> tuple[bool, list[str]]:
    errors: list[str] = []
    missing = [field for field in required_fields if field not in payload]
    if missing:
        errors.append(f"missing_fields={','.join(missing)}")
    
    # å­—ç¬¦ä¸²æ¯”è¾ƒï¼
    if str(payload.get("failure_queue_snapshot_ref")) != expected_failure_ref:
        errors.append("failure_queue_snapshot_ref does not match")
```

**é—®é¢˜**:

1. **æ— Schemaçº¦æŸ** - failure_refæ ¼å¼ä»…é€šè¿‡æ­£åˆ™æ ¡éªŒï¼Œæ— JSON Schemaå¼ºåˆ¶
   ```
   sqlite://output/workpackages/db.db#failure_queue  âœ… pass
   sqlite://output/workpackages/db.db#failure_run    âŒ should fail (wrong table)ï¼Œä½†regexæ”¯æŒä»»æ„è¡¨å
   ```

2. **è¿è¡Œæ—¶æ ¡éªŒè€Œéç¼–è¯‘æ—¶** - é”™è¯¯å‘ç°æ—¶å·²äº§ç”Ÿartifacts
   ```
   # è¿è¡Œåˆ°line_feedbackç”Ÿæˆåæ‰æ ¡éªŒï¼Œæ— æ³•retroactive fix
   # åº”è¯¥åœ¨ç¼–è¯‘dispatch-006æ—¶å°±validate
   ```

3. **æ— ç‰ˆæœ¬åŒ–** - line_feedback.latestæ²¡æœ‰schemaç‰ˆæœ¬æ ‡è®°
   ```json
   {
     "failure_queue_snapshot_ref": "sqlite://...",
     // ç¼ºå°‘ç‰ˆæœ¬å·ï¼Œæ— æ³•forward-compat
     // è‹¥è¿ç§»sqliteåˆ°postgresï¼Œæ—§çš„line_feedbackæ€ä¹ˆç†è§£ï¼Ÿ
   }
   ```

4. **SQLiteå¼•ç”¨ç¡¬ç¼–ç ** - å‡è®¾æ‰€æœ‰feedbackéƒ½ç”¨SQLiteï¼Œæ— æ³•æ‰©å±•åˆ°postgres/s3ç­‰
   ```python
   SQLITE_REF_RE = re.compile(r"^sqlite://(?P<path>[^#]+)#(?P<table>[A-Za-z_][A-Za-z0-9_]*)$")
   # è‹¥è¦æ”¯æŒ "postgres://..." æˆ– "s3://..."ï¼Œéœ€ä¿®æ”¹æ­£åˆ™+parser+validator
   ```

**å»ºè®®æ–¹æ¡ˆ**:
```python
# contracts/line_feedback_contract_v2.json (JSON Schema)
{
  "$schema": "https://json-schema.org/draft/2020-12",
  "title": "LineFeedbackContract v2",
  "type": "object",
  "properties": {
    "version": {
      "const": "2",
      "description": "Contract version for forward-compat"
    },
    "failure_queue_snapshot_ref": {
      "type": "string",
      "oneOf": [
        {
          "pattern": "^sqlite://[^#]+#failure_queue$"
        },
        {
          "pattern": "^postgres://[^#]+#failure_queue$"
        },
        {
          "pattern": "^s3://[^/]+/[^#]+#failure_queue$"
        }
      ],
      "description": "Storage backend for failure snapshots (sqlite|postgres|s3)"
    },
    "replay_result_ref": {
      "type": "string",
      "pattern": "^(sqlite|postgres)://[^#]+#replay_runs$"
    },
    "evidence_refs": {
      "type": "array",
      "items": {"type": "string", "pattern": "^(file|s3|http)://"},
      "minItems": 1
    },
    "schema_hash": {
      "type": "string",
      "pattern": "^[0-9a-f]{64}$",
      "description": "SHA256 of schema at time of contract generation"
    }
  },
  "additionalProperties": false,
  "required": [
    "version",
    "failure_queue_snapshot_ref",
    "replay_result_ref",
    "evidence_refs",
    "schema_hash"
  ]
}

# åœ¨ dispatch-006 çš„éªŒæ”¶æ¸…å•ä¸­
# å¢åŠ : line_feedback_schema_validation âœ“
jsonschema.validate(line_feedback_payload, schema=contract_v2_schema)
```

**ä¼˜åŒ–æ”¶ç›Š**:
- âœ… ç¼–è¯‘æ—¶å‘ç°é”™è¯¯ï¼ˆdispatchç”Ÿæˆæ—¶ç«‹å³validateï¼‰
- âœ… æ‰©å±•æ€§ï¼ˆæ”¯æŒå¤šä¸ªå­˜å‚¨åç«¯ï¼‰
- âœ… å‘å‰å…¼å®¹ï¼ˆé€šè¿‡versionæ§åˆ¶ï¼Œv3å¯ä»¥deprecate v2çš„æŸå­—æ®µï¼‰

---

### 5. **ç¼“å­˜ç­–ç•¥çš„è‰ç‡æ€§ - "allowed_use_notes"æ— æ‰§è¡ŒåŠ›** ğŸŸ¢ ä½é£é™©ï¼ˆå½±å“é¢å°ï¼‰

**å½“å‰ä»£ç ** (trust_repository.py L204, L220):
```python
{
    "allowed_use_notes": "cache allowed for internal governance",
    # æ²¡æœ‰äººåœ¨è¿è¡Œæ—¶æ£€æŸ¥è¿™ä¸ªnoteï¼
}

{
    "allowed_use_notes": "cache allowed with attribution",
    # attributionåœ¨å“ªé‡Œå®ç°ï¼Ÿä»£ç æ²¡æ‰¾åˆ°
}
```

**é—®é¢˜**:
- ç¼“å­˜ç­–ç•¥ä»…ä¸ºæ–‡æœ¬è¯´æ˜ï¼Œæ— enforcing logic
- è‹¥åç»­å¼•å…¥çœŸå®ç¼“å­˜å±‚ï¼ˆRedis)ï¼Œå®¹æ˜“é—æ¼æŸäº›å­—æ®µçš„ç¼“å­˜ç¦æ­¢
- TTLä¸æ˜ç¡®ï¼ˆå†…å­˜ç¼“å­˜è¿˜æ˜¯åˆ†å¸ƒå¼ç¼“å­˜ï¼Ÿï¼‰

**å»ºè®®æ–¹æ¡ˆ**:
```python
# å£°æ˜å¼ç¼“å­˜ç­–ç•¥
@dataclass
class CachePolicy:
    enable: bool
    ttl_sec: int
    key_pattern: str
    conditions: List[str]  # e.g., ["if user == 'internal'", "if source != 'external'"]

# åœ¨ trust_repository.py
CACHE_POLICIES = {
    "addr_canonical": CachePolicy(
        enable=True,
        ttl_sec=3600,
        key_pattern="addr:canonical:{raw_id}",
        conditions=["user in ['internal_governance', 'lab_system']"],
    ),
    "ruleset_evaluation": CachePolicy(
        enable=True,
        ttl_sec=300,
        key_pattern="rule:{ruleset_id}:{input_hash}",
        conditions=["complexity < 50", "retry_count < 2"],
    ),
}

# è¿è¡Œæ—¶cache decorator
def with_cache_policy(policy: CachePolicy):
    def decorator(func):
        async def wrapper(*args, **kwargs):
            key = policy.key_pattern.format(**kwargs)
            if cached := await redis.get(key):
                return cached
            
            result = await func(*args, **kwargs)
            if all(condition.evaluate(**kwargs) for condition in policy.conditions):
                await redis.setex(key, policy.ttl_sec, result)
            return result
        return wrapper
    return decorator

@with_cache_policy(CACHE_POLICIES["addr_canonical"])
async def get_canonical_for_raw(raw_id: str, user: str):
    ...
```

**ä¼˜åŒ–æ”¶ç›Š**:
- âœ… ç¼“å­˜ç­–ç•¥å¯å®¡è®¡ï¼ˆæ˜¾å¼çš„whitelistï¼‰
- âœ… TTLæ˜ç¡®ï¼ˆä¸å†éšå«ï¼‰
- âœ… æ¡ä»¶cacheï¼ˆä»…åœ¨safeæƒ…å†µä¸‹ç¼“å­˜ï¼‰

---

### 6. **Web Dashboard çš„è·¨åŸŸå’ŒCSPé£é™©** ğŸŸ¡ ä¸­é£é™©

**å½“å‰ä»£ç ** (web/dashboard/index.html):
```html
<!doctype html>
<html lang="zh-CN">
  <head>
    <meta charset="UTF-8" />
    <meta name="viewport" content="width=device-width, initial-scale=1.0" />
    <title>é¡¹ç›®ç®¡ç†é©¾é©¶èˆ±</title>
    <link rel="stylesheet" href="/static/styles.css" />
    <!-- ç¼ºå°‘CSP header! -->
  </head>
```

**web/dashboard/app.js æ•°æ®è·å–æ–¹å¼**:
```javascript
async function readJson(file) {
  const res = await fetch(`/data/${file}`, { cache: 'no-store' });
  if (!res.ok) throw new Error(`load failed: ${file}`);
  return res.json();
}
```

**é—®é¢˜**:
1. **æ— CSP Header** - æµè§ˆå™¨å…è®¸evalå’Œinline scriptï¼ŒXSSé£é™©
2. **åŠ¨æ€fetchè·¯å¾„** - `readJson('dashboard_manifest.json')` è‹¥manifestè¢«æ³¨å…¥æ¶æ„è·¯å¾„ï¼Œå¯èƒ½åŠ è½½ä»»æ„æ•°æ®
3. **data-line JSONå°è£…** (renderWorklines):
   ```javascript
   const packed = encodeURIComponent(JSON.stringify({...}));
   // åœ¨hiddençš„dataå±æ€§ä¸­å­˜å‚¨JSONï¼Œå¯èƒ½è¢«XPath/DOMéå†è·å–
   ```

**å»ºè®®æ–¹æ¡ˆ**:
```html
<!-- 1. CSP Header -->
<meta http-equiv="Content-Security-Policy" content="
  default-src 'self';
  script-src 'self';
  style-src 'self' 'unsafe-inline';
  img-src 'self' data:;
  font-src 'self';
  connect-src 'self';
  frame-ancestors 'none';
" />

<!-- 2. SRI (Subresource Integrity) å¯¹æ‰€æœ‰å¤–éƒ¨èµ„æº -->
<link rel="stylesheet" href="/static/styles.css" integrity="sha384-..." />
```

```javascript
// 3. Sanitize JSON in DOM
function renderWorklines(rows, dispatchIndex = {}) {
  el.worklines.innerHTML = (rows || []).map((x) => {
    // ä¸è¦å­˜å‚¨åœ¨data attributeï¼Œæ”¹ç”¨WeakMap
    const dataKey = Symbol(`workline:${x.line_id}`);
    worklineDataStore.set(el, dataKey, {
      line_name: x.line_name,
      owner: x.owner,
      // ...
    });
    
    // buttonåªåŒ…å«key reference
    return `
      <tr>
        <td>${escapeHtml(val(x.line_name))}</td>
        ...
        <button data-workline-key="${dataKey}">ä»»åŠ¡è¯¦æƒ…</button>
      </tr>
    `;
  }).join('');
}

// ç‚¹å‡»æ—¶ä»WeakMapå–æ•°æ®ï¼Œè€ŒéDOMååºåˆ—åŒ–
```

**ä¼˜åŒ–æ”¶ç›Š**:
- âœ… XSSé£é™©ä»Highé™ä½åˆ°Low
- âœ… DOMæ•°æ®ä¸å¯è¢«ä¾§ä¿¡é“è®¿é—®
- âœ… SRIç¡®ä¿èµ„æºå®Œæ•´æ€§ï¼ˆtamper-resistant CDN deliveryï¼‰

---

### 7. **å¯è§‚æµ‹æ€§ï¼šæŒ‡æ ‡ç¼ºå¤±ä¸èšåˆç‚¹ä¸æ¸…** ğŸŸ¡ ä¸­é£é™©

**å½“å‰äº§å‡ºç‰©ç»Ÿè®¡**:

| ç»„ä»¶ | æŒ‡æ ‡ç±»å‹ | è¦†ç›–åº¦ | ç¼ºå¤± |
|------|---------|--------|------|
| web_e2e test | duration, pass/fail | âœ… 70% | æµè§ˆå™¨å†…å­˜æ¶ˆè€—ã€DOMæ¸²æŸ“æ—¶é—´ |
| SQL query | elapsed_ms, rows | âœ… 80% | query plan explain, cache hit rate |
| line_feedback | event count | âŒ 0% | feedbackç”Ÿæˆå»¶è¿Ÿã€éªŒè¯å¤±è´¥cause |
| address normalize | score | âœ… 50% | match latency distribution, p99 |

**é—®é¢˜**:
1. **æŒ‡æ ‡æ— èšåˆç­–ç•¥** - å„çº¿ç‹¬ç«‹äº§ç”Ÿæ—¥å¿—ï¼Œæ— ä¸­å¤®æ”¶é›†
2. **è§‚æµ‹æ€§åˆ†æ•£** - æ— ç»Ÿä¸€çš„trace correlation IDï¼Œéš¾ä»¥è¿½è¸ªè·¨æœåŠ¡è¯·æ±‚
3. **alertç¼ºå¤±** - è´¨é‡é—¨æ§›çœ‹èµ·æ¥æ˜¯passï¼Œä½†æ— å…³é”®æŒ‡æ ‡çš„SLO alert

**å»ºè®®æ–¹æ¡ˆ**:
```python
# ç»Ÿä¸€æŒ‡æ ‡æ¨¡å‹
@dataclass
class ObservabilityEvent:
    trace_id: str  # UUIDï¼Œä¼ æ’­æ•´ä¸ªè¯·æ±‚é“¾
    span_id: str   # å½“å‰operation
    parent_span_id: Optional[str]
    service: str
    operation: str
    status: str  # "ok" | "error" | "timeout"
    duration_ms: float
    timestamp: datetime
    
    # æœåŠ¡ç‰¹å®šmetrics
    metrics: Dict[str, Any]  # {"query_ms": 120, "rows": 45, ...}
    errors: List[Dict]

# åœ¨å…³é”®è·¯å¾„ä¸Šæ³¨å…¥è§‚æµ‹
async def observed(service: str, op: str):
    """Context manager for observation"""
    trace_id = context.get("trace_id") or str(uuid.uuid4())
    span_id = str(uuid.uuid4())
    
    started = time.time()
    try:
        yield ObservationContext(trace_id, span_id)
        status = "ok"
    except TimeoutError:
        status = "timeout"
        raise
    except Exception as e:
        status = "error"
        raise
    finally:
        duration_ms = (time.time() - started) * 1000
        event = ObservabilityEvent(
            trace_id=trace_id,
            span_id=span_id,
            service=service,
            operation=op,
            status=status,
            duration_ms=duration_ms,
            timestamp=datetime.now(timezone.utc),
        )
        # å‘é€åˆ°ä¸­å¤®observability sink (Datadog/New Relic/OpenTelemetry)
        await telemetry_client.record(event)

# ä½¿ç”¨
async def get_optimize(req):
    async with observed("governance_api", "post_optimize") as obs:
        obs.metrics["input_size"] = len(req.address)
        result = await optimize(req.address)
        obs.metrics["output_size"] = len(result.normalized)
        return result

# å‘Šè­¦è§„åˆ™ (SLO YAML)
alerts:
  - name: "sql_query_p99_slo"
    condition: |
      histogram_quantile(0.99, 
        rate(sql_query_duration_ms[5m])) > 1500
    severity: warning
    action: "page_on_call"
  
  - name: "line_feedback_validation_failure_rate"
    condition: |
      rate(line_feedback_validation_failed[5m]) > 0.01  # 1%
    severity: warning
    action: "page_oncall"
```

**ä¼˜åŒ–æ”¶ç›Š**:
- âœ… ç«¯åˆ°ç«¯trace visibilityï¼ˆå¯è¿½æº¯user request â†’ optimize â†’ trust_hub â†’ external_apiï¼‰
- âœ… SLO-driven alertsï¼ˆè¿åSLOæ‰å‘Šè­¦ï¼Œå‡å°‘noiseï¼‰
- âœ… æ€§èƒ½profileï¼ˆçŸ¥é“bottleneckåœ¨å“ªï¼‰

---

## ğŸ¯ ä¼˜å…ˆçº§ä¸è¡ŒåŠ¨è®¡åˆ’

### ç«‹å³è¡ŒåŠ¨ï¼ˆIteration-007ï¼Œ1-2å‘¨ï¼‰
| åºå· | é£é™©ç±»å‹ | å»ºè®®æªæ–½ | æ‰€æœ‰è€… | ETA |
|-----|---------|--------|--------|-----|
| 1 | Health calcç¼ºé™· | æ”¹è¿›KPIå…¬å¼ï¼Œå¼•å…¥progressè´¡çŒ®åº¦æå‡ | çœ‹æ¿ç ”å‘çº¿ | 2/23 |
| 2 | Timeoutç¢ç‰‡åŒ– | ç»Ÿä¸€timeout policy+SLA budgetåˆ†é… | æ ¸å¿ƒå¼•æ“çº¿ | 2/23 |
| 3 | ç¼“å­˜strategyæ–‡æœ¬åŒ– | å®ç°å£°æ˜å¼ç¼“å­˜enforcer | äº§çº¿æ‰§è¡Œçº¿ | 2/20 |

### ä¸­æœŸè®¡åˆ’ï¼ˆIteration-008-009ï¼Œ3-4å‘¨ï¼‰
| åºå· | é£é™©ç±»å‹ | å»ºè®®æªæ–½ | æ‰€æœ‰è€… | ETA |
|-----|---------|--------|--------|-----|
| 4 | DBæ¶æ„ç¼ºä¹åŒæ­¥ | å®ç°Event Sourcingï¼Œå†·çƒ­å±‚æœ€ç»ˆä¸€è‡´ | æ ¸å¿ƒå¼•æ“çº¿+Hubçº¿ | 3/2 |
| 5 | line_feedbackè„†å¼± | å¼•å…¥JSON Schemaç‰ˆæœ¬åŒ–+ç¼–è¯‘æ—¶éªŒè¯ | äº§çº¿æ‰§è¡Œçº¿+æ€»æ§ | 2/28 |
| 6 | è§‚æµ‹æ€§åˆ†æ•£ | ç»Ÿä¸€telemetry/tracingåŸºç¡€è®¾æ–½ | å¯è§‚æµ‹çº¿+æ ¸å¿ƒå¼•æ“çº¿ | 3/5 |

### åç»­ä¼˜åŒ–ï¼ˆDesign Debtï¼‰
| åºå· | é£é™©ç±»å‹ | å»ºè®®æªæ–½ | å½±å“ |
|-----|---------|--------|------|
| 7 | Web CSP/XSS | å®ç°å®Œæ•´CSPç­–ç•¥+SRI | å®‰å…¨æ€§æå‡ |
| 8 | SQLite scale limit | è¿ç§»runtime stateåˆ°Postgres | æ”¯æŒ100xå¹¶å‘ |

---

## ğŸ“Š æŠ€æœ¯å€ºè¯„ä¼°

**å½“å‰é¡¹ç›®çš„æŠ€æœ¯å€ºåŠ¡ç­‰çº§**: ğŸŸ¡ **ä¸­ç­‰** (å¯æ§)

```
é«˜é£é™©åŒºï¼ˆéœ€ç´§æ€¥å¤„ç†ï¼‰:
  â–¡ 0 é¡¹

ä¸­é£é™©åŒºï¼ˆéœ€è¦è§„åˆ’ï¼‰:
  â”œâ”€ å¥åº·åº¦è®¡ç®—å…¬å¼æ”¹è¿› 
  â”œâ”€ æ•°æ®åº“åŒæ­¥ä¿è¯
  â”œâ”€ Timeoutç»Ÿä¸€ç­–ç•¥
  â”œâ”€ line_feedback schemaç‰ˆæœ¬åŒ–
  â””â”€ å¯è§‚æµ‹æ€§æ¶æ„

ä½é£é™©åŒºï¼ˆåç»­è¿­ä»£ï¼‰:
  â”œâ”€ ç¼“å­˜enforcerå®ç°
  â”œâ”€ Web CSPåŠ å›º
  â””â”€ SQLiteæ€§èƒ½ä¼˜åŒ–
```

**æŠ€æœ¯å€ºå¯¹å‘å¸ƒçš„å½±å“**: âœ… **æ— é˜»å¡** - æ‰€æœ‰é£é™©éƒ½æ˜¯å¯æ§çš„ï¼Œä¸å½±å“å½“å‰dispatch-006çš„GOå†³ç­–ã€‚

---

## ç»“è®ºä¸å»ºè®®

### âœ… å‘å¸ƒæ˜¯å®‰å…¨çš„ï¼Œä½†éœ€è¦åç»­å·¥ç¨‹æŠ•å…¥

**æœ¬æ‰¹æ¬¡ï¼ˆdispatch-006ï¼‰ä¸å­˜åœ¨é˜»å¡å‘å¸ƒçš„æŠ€æœ¯é£é™©**ã€‚æ‰€æœ‰å‘ç°çš„é—®é¢˜éƒ½æ˜¯ï¼š
1. é•¿æœŸå¯é æ€§é—®é¢˜ï¼ˆDBåŒæ­¥ã€ç›‘æµ‹ç¼ºå¤±ï¼‰
2. å¯ç”¨æ€§é—®é¢˜ï¼ˆtimeouté…ç½®ã€health indicatorï¼‰
3. å¯ç»´æŠ¤æ€§é—®é¢˜ï¼ˆline_feedback contractç‰ˆæœ¬åŒ–ï¼‰

è¿™äº›é—®é¢˜é€‚åˆåœ¨åç»­è¿­ä»£ä¸­æœ‰ä¼˜å…ˆçº§åœ°å¤„ç†ï¼Œä¸åº”è¯¥å»¶è¿Ÿå½“å‰å‘å¸ƒã€‚

### ğŸ¯ å…³é”®å»ºè®®

1. **å¥åº·åº¦å…¬å¼æ”¹è¿›** - ä½æˆæœ¬ï¼Œé«˜ROIï¼ˆæ”¹å–„PMå†³ç­–çš„å‡†ç¡®æ€§ï¼‰
2. **Timeout SLAæ–‡æ¡£åŒ–** - ä¸­ç­‰æˆæœ¬ï¼Œå¿…éœ€ï¼ˆé¿å…æœªæ¥çš„distributed system headachesï¼‰
3. **Event Sourcingæ¶æ„** - é«˜æˆæœ¬ï¼Œä½†æ˜¯æˆ˜ç•¥æŠ•å…¥ï¼ˆä¸ºfuture scaleåšå‡†å¤‡ï¼‰
4. **å¯è§‚æµ‹æ€§ç»Ÿä¸€** - ä¸­ç­‰æˆæœ¬ï¼Œcritical for production stability

### ğŸ“‹ å»ºè®®æäº¤ä¸ºIteration-008çš„è®¾è®¡ä»»åŠ¡

å°†æœ¬æŠ¥å‘Šçš„å„ä¸ªä¼˜åŒ–æ–¹æ¡ˆåˆ†åˆ«æ‹†è§£ä¸ºï¼š
- **æŠ€æœ¯è®¾è®¡æ–‡æ¡£** (design specs)
- **æœ€å°å®ç°ç‰ˆæœ¬** (MVP)
- **éªŒæ”¶æ ‡å‡†** (checklist)

è¿™æ ·å¯ä»¥åœ¨åç»­è¿­ä»£ä¸­ç³»ç»Ÿæ¨è¿›ï¼Œè€Œä¸è‡³äºæ²¦ä¸ºtechnical debtã€‚

